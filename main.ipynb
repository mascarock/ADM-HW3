{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieslist = []\n",
    "\n",
    "for tr in soup.findAll('tr'):\n",
    "    for td in tr.findAll('td'):\n",
    "        if \"http\" in (td.text):\n",
    "            movieslist.append(td.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10000):\n",
    "    time.sleep(3)\n",
    "    response = urlopen(movieslist[i])\n",
    "    namefile = \"article_\" + str(i)+ \".html\"\n",
    "    with open(namefile, \"wb\") as f:\n",
    "        f.write(response.read())\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  read .HTML files\n",
    "folder_dir = os.listdir('Html-data')\n",
    "for i in range(1,10000):\n",
    "    htmlfile=open(r'Html-data/article_'+str(i)+'.html','r',encoding='utf8')\n",
    "    soup=BeautifulSoup(htmlfile,'html.parser')\n",
    "    \n",
    "    #Find Title of movies\n",
    "    tsv_content=[]\n",
    "    dict_content={}\n",
    "    dict_content['title']=soup.select_one('h1',{\"class\":\"firstheading:\"}).text \n",
    "    \n",
    "    # Find intro of movies\n",
    "    \n",
    "    pr_content = soup.select('p')\n",
    "    try:\n",
    "        dict_content['intro']=\"\".join(pr_content[0].text.strip(\"\\n\"))\n",
    "    except:\n",
    "        dict_content['intro']=\"NA\"\n",
    "    \n",
    "    # Find plot of movies\n",
    "    try:\n",
    "        dict_content['plot']=\"\".join(pr_content[1].text.strip(\"\\n\"))\n",
    "    except:\n",
    "        dict_content['plot']=\"NA\"\n",
    "        \n",
    "    for data in dict_content:\n",
    "        tsv_content.append(dict_content[data])\n",
    "        \n",
    "    # Find infobox content \n",
    "    table_body=soup.find('tbody')\n",
    "    if table_body!= None:\n",
    "        allrows = table_body.find_all('tr')\n",
    "        for rows in allrows:\n",
    "            if rows.th!= None:\n",
    "                if rows.th.text == 'Directed by':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Produced by':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Written by':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Starring':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Music by':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Release date':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Running time':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Country':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text == 'Language':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\")))\n",
    "                elif rows.th.text =='Budget':\n",
    "                    tsv_content.append(\"\".join(rows.td.text.strip(\"\\n\"))) \n",
    "                \n",
    "    # info saved into different .tsv files  \n",
    "    \n",
    "    with open('TSV-data/article_'+str(i)+'.tsv', 'wt') as out_file:\n",
    "        try:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(tsv_content)\n",
    "        except:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow('This page not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to clean all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(string):\n",
    "    \n",
    "    sentence1 = str(string).lower() # converting all in lower character to don't have double characters\n",
    "    sentence2 = ''.join([i for i in sentence1 if not i.isdigit()]) # removing all the numbers\n",
    "    \n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(sentence2)\n",
    "    no_stopwords = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    # removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    no_punctuation = tokenizer.tokenize(str(no_stopwords))\n",
    "\n",
    "    # stemming data\n",
    "    ps = PorterStemmer() \n",
    "    stemming = []  \n",
    "    for w in no_punctuation:\n",
    "        stemming.append(ps.stem(w))\n",
    "    stemmed_data = list(set(stemming)) # taking a set to take single word only 1 time\n",
    "    \n",
    "    return stemmed_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love by the light of the moon\n"
     ]
    }
   ],
   "source": [
    "query = input()  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'light', 'moon']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(query) # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Conjunctive query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll take all the words from intro and plot of each document. In tsv files we already have only intro and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary to put all the words from the tsv files (10.000 - 20.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "d = []\n",
    "\n",
    "for i in range(10000, 20000):\n",
    "    try:\n",
    "        with open(path+\"\\\\tsvfile\\\\article_\"+str(i)+\".tsv\", newline=\"\", encoding=\"ISO-8859-1\") as tsv_file:\n",
    "                # Loop through each line of the file \n",
    "            for line in tsv_file:\n",
    "                # Removing the leading spaces and newline character \n",
    "                line = line.strip() \n",
    "                \n",
    "                # Convert the characters in line to  \n",
    "                # lowercase to avoid case mismatch \n",
    "                line = line.lower() \n",
    "\n",
    "                # Split the line into words \n",
    "                words = line.split(\" \") \n",
    "                d.append(line)\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I assigne a variable \"data\" that contain in a list all the words filtered by the function filtering() created previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtering(\"\".join(d))  # in this way I get all the words in a single string and then I apply the filter at this string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119771"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # showing how many different words there are in the files from 10.000 to 20.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary that maps each word to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {k: v for v, k in enumerate(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
